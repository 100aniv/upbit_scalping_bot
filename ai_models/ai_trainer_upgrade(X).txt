import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
import joblib

class RandomForestModel:
    def __init__(self, n_estimators=100, max_depth=None, random_state=42):
        """
        랜덤 포레스트 모델 초기화
        :param n_estimators: 트리의 개수
        :param max_depth: 트리의 최대 깊이
        :param random_state: 랜덤 시드 값
        """
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=random_state
        )
        self.scaler = StandardScaler()  # 데이터 스케일링을 위한 StandardScaler

    def train(self, X, y):
        """
        랜덤 포레스트 모델 학습
        :param X: 학습 데이터 (입력 특성)
        :param y: 학습 데이터 (타겟 값)
        """
        # 데이터를 스케일링 (특성을 표준화)
        X_scaled = self.scaler.fit_transform(X)
        self.model.fit(X_scaled, y)

    def predict(self, X):
        """
        입력 데이터에 대한 예측 수행
        :param X: 입력 데이터
        :return: 예측 결과
        """
        # 스케일링 후 예측
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)

    def evaluate(self, X, y):
        """
        모델 평가
        :param X: 테스트 데이터
        :param y: 실제 값 (정답)
        :return: 정확도, 정밀도, 재현율, F1 점수
        """
        y_pred = self.predict(X)  # 예측값
        accuracy = accuracy_score(y, y_pred)
        precision = precision_score(y, y_pred, average='weighted')
        recall = recall_score(y, y_pred, average='weighted')
        f1 = f1_score(y, y_pred, average='weighted')

        # 평가 결과 출력
        print(f"정확도: {accuracy:.4f}")
        print(f"정밀도: {precision:.4f}")
        print(f"재현율: {recall:.4f}")
        print(f"F1 점수: {f1:.4f}")

        return accuracy, precision, recall, f1

    def save_model(self, model_path, scaler_path):
        """
        모델 저장
        :param model_path: 모델 저장 경로
        :param scaler_path: 스케일러 저장 경로
        """
        joblib.dump(self.model, model_path)
        joblib.dump(self.scaler, scaler_path)
        print(f"모델이 {model_path}에 저장되었습니다.")
        print(f"스케일러가 {scaler_path}에 저장되었습니다.")

    def load_model(self, model_path, scaler_path):
        """
        저장된 모델 및 스케일러 로드
        :param model_path: 모델 파일 경로
        :param scaler_path: 스케일러 파일 경로
        """
        self.model = joblib.load(model_path)
        self.scaler = joblib.load(scaler_path)
        print(f"모델이 {model_path}에서 로드되었습니다.")
        print(f"스케일러가 {scaler_path}에서 로드되었습니다.")


if __name__ == "__main__":
    # 예제 데이터 생성
    from sklearn.datasets import load_iris
    data = load_iris()
    X, y = data.data, data.target

    # 학습 데이터와 테스트 데이터 분리
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 랜덤 포레스트 모델 초기화 및 학습
    rf_model = RandomForestModel(n_estimators=200, max_depth=10)
    rf_model.train(X_train, y_train)

    # 테스트 데이터 평가
    print("\n모델 평가 결과:")
    rf_model.evaluate(X_test, y_test)

    # 모델 저장
    rf_model.save_model("random_forest_model.pkl", "random_forest_scaler.pkl")

    # 모델 로드 후 예측 테스트
    loaded_model = RandomForestModel()
    loaded_model.load_model("random_forest_model.pkl", "random_forest_scaler.pkl")
    predictions = loaded_model.predict(X_test)
    print("\n로딩된 모델의 첫 5개 예측값:", predictions[:5])
